"""
Comprehensive Weather Data Cleaning Script for Indian Weather Dataset.

This script performs thorough data cleaning on the CSV file generated by claudemain.py,
handling edge cases, outliers, and ensuring data quality for analysis.
Follows PEP 8 standards with comprehensive error handling and documentation.
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
from typing import Dict, Optional
from datetime import datetime
import warnings

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Suppress pandas warnings for cleaner output
warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)


class WeatherDataCleaner:
    """
    A comprehensive weather data cleaning class that handles various data quality issues.
    
    This class provides methods to clean, validate, and standardize weather data
    with proper error handling and edge case management.
    """
    
    def __init__(self, input_file: str, output_file: str = None):
        """
        Initialize the WeatherDataCleaner.
        
        Args:
            input_file: Path to the input CSV file
            output_file: Path to the output CSV file (optional)
        """
        self.input_file = Path(input_file)
        self.output_file = Path(output_file) if output_file else self._generate_output_filename()
        self.data: Optional[pd.DataFrame] = None
        self.cleaning_report: Dict = {}
        
        # Define realistic weather parameter ranges for India
        self.valid_ranges = {
            'Temperature_C': (-20, 60),           # Extreme cold to record heat
            'Min_Temperature_C': (-25, 55),      # Account for mountain regions
            'Max_Temperature_C': (-15, 65),      # Record temperatures possible
            'Precipitation_mm': (0, 1000),       # Max daily rainfall recorded
            'Humidity_percent': (5, 100),        # Desert minimum to maximum
            'Wind_Speed_kmh': (0, 200),          # Calm to cyclonic winds
            'Atmospheric_Pressure_hPa': (850, 1100),  # High altitude to sea level
            'Heat_Index_C': (-30, 200)           # Extreme cold to heat index
        }
        
        # Define Indian states and UTs for validation
        self.valid_regions = {
            # States
            'Andhra Pradesh', 'Arunachal Pradesh', 'Assam', 'Bihar', 'Chhattisgarh',
            'Goa', 'Gujarat', 'Haryana', 'Himachal Pradesh', 'Jharkhand',
            'Karnataka', 'Kerala', 'Madhya Pradesh', 'Maharashtra', 'Manipur',
            'Meghalaya', 'Mizoram', 'Nagaland', 'Odisha', 'Punjab', 'Rajasthan',
            'Sikkim', 'Tamil Nadu', 'Telangana', 'Tripura', 'Uttar Pradesh',
            'Uttarakhand', 'West Bengal',
            # Union Territories
            'Andaman and Nicobar Islands', 'Chandigarh', 
            'Dadra and Nagar Haveli and Daman and Diu', 'Delhi',
            'Jammu and Kashmir', 'Ladakh', 'Lakshadweep', 'Puducherry'
        }

    def _generate_output_filename(self) -> Path:
        """Generate output filename with timestamp."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return self.input_file.parent / f"{self.input_file.stem}_cleaned_{timestamp}.csv"

    def load_data(self) -> bool:
        """
        Load data from CSV file with comprehensive error handling.
        
        Returns:
            bool: True if data loaded successfully, False otherwise
            
        Raises:
            FileNotFoundError: If input file doesn't exist
            pd.errors.EmptyDataError: If CSV file is empty
        """
        try:
            if not self.input_file.exists():
                raise FileNotFoundError(f"Input file not found: {self.input_file}")
            
            logger.info(f"Loading data from: {self.input_file}")
            self.data = pd.read_csv(self.input_file)
            
            if self.data.empty:
                raise pd.errors.EmptyDataError("CSV file is empty")
            
            logger.info(f"Successfully loaded {len(self.data)} records with {len(self.data.columns)} columns")
            return True
            
        except FileNotFoundError as e:
            logger.error(f"File not found: {e}")
            return False
        except pd.errors.EmptyDataError as e:
            logger.error(f"Empty data error: {e}")
            return False
        except Exception as e:
            logger.error(f"Unexpected error loading data: {e}")
            return False

    def validate_data_structure(self) -> bool:
        """
        Validate the basic structure and columns of the dataset.
        
        Returns:
            bool: True if structure is valid, False otherwise
        """
        logger.info("Validating data structure...")
        
        expected_columns = [
            'Region', 'Date', 'Temperature_C', 'Precipitation_mm', 
            'Humidity_percent', 'Min_Temperature_C', 'Max_Temperature_C',
            'Wind_Speed_kmh', 'Atmospheric_Pressure_hPa', 'Heat_Index_C'
        ]
        
        missing_columns = set(expected_columns) - set(self.data.columns)
        extra_columns = set(self.data.columns) - set(expected_columns)
        
        if missing_columns:
            logger.error(f"Missing columns: {missing_columns}")
            return False
        
        if extra_columns:
            logger.warning(f"Extra columns found: {extra_columns}")
        
        self.cleaning_report['structure_validation'] = {
            'expected_columns': len(expected_columns),
            'actual_columns': len(self.data.columns),
            'missing_columns': list(missing_columns),
            'extra_columns': list(extra_columns)
        }
        
        logger.info("Data structure validation completed")
        return True

    def handle_missing_values(self) -> None:
        """
        Handle missing values with appropriate strategies for each column type.
        """
        logger.info("Handling missing values...")
        
        initial_missing = self.data.isnull().sum().sum()
        missing_by_column = self.data.isnull().sum()
        
        # Log missing values by column
        for col, count in missing_by_column.items():
            if count > 0:
                logger.warning(f"Missing values in {col}: {count}")
        
        # Handle missing values by column type
        numeric_columns = self.data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            if col in missing_by_column and missing_by_column[col] > 0:
                # Use forward fill for time series data, then backward fill
                self.data[col] = self.data.groupby('Region')[col].fillna(method='ffill')
                self.data[col] = self.data.groupby('Region')[col].fillna(method='bfill')
                
                # If still missing, use median by region
                remaining_missing = self.data[col].isnull().sum()
                if remaining_missing > 0:
                    median_by_region = self.data.groupby('Region')[col].median()
                    self.data[col] = self.data[col].fillna(
                        self.data['Region'].map(median_by_region)
                    )
        
        # Handle missing dates
        if 'Date' in missing_by_column and missing_by_column['Date'] > 0:
            logger.warning("Found missing dates - removing rows with missing dates")
            self.data = self.data.dropna(subset=['Date'])
        
        # Handle missing regions
        if 'Region' in missing_by_column and missing_by_column['Region'] > 0:
            logger.warning("Found missing regions - removing rows with missing regions")
            self.data = self.data.dropna(subset=['Region'])
        
        final_missing = self.data.isnull().sum().sum()
        
        self.cleaning_report['missing_values'] = {
            'initial_missing': int(initial_missing),
            'final_missing': int(final_missing),
            'missing_handled': int(initial_missing - final_missing)
        }
        
        logger.info(f"Missing values handled: {initial_missing} → {final_missing}")

    def validate_date_format(self) -> None:
        """
        Validate and standardize date format.
        """
        logger.info("Validating date format...")
        
        try:
            # Convert to datetime
            self.data['Date'] = pd.to_datetime(self.data['Date'], errors='coerce')
            
            # Check for invalid dates
            invalid_dates = self.data['Date'].isnull().sum()
            if invalid_dates > 0:
                logger.warning(f"Found {invalid_dates} invalid dates - removing these rows")
                self.data = self.data.dropna(subset=['Date'])
            
            # Add useful date components
            self.data['Year'] = self.data['Date'].dt.year
            self.data['Month'] = self.data['Date'].dt.month
            self.data['Day'] = self.data['Date'].dt.day
            self.data['DayOfYear'] = self.data['Date'].dt.dayofyear
            
            # Validate date range (should be 2024)
            min_date = self.data['Date'].min()
            max_date = self.data['Date'].max()
            
            logger.info(f"Date range: {min_date.date()} to {max_date.date()}")
            
            self.cleaning_report['date_validation'] = {
                'invalid_dates_removed': int(invalid_dates),
                'date_range': f"{min_date.date()} to {max_date.date()}",
                'total_days': (max_date - min_date).days + 1
            }
            
        except Exception as e:
            logger.error(f"Error in date validation: {e}")
            raise

    def validate_regions(self) -> None:
        """
        Validate region names against known Indian states and UTs.
        """
        logger.info("Validating region names...")
        
        unique_regions = set(self.data['Region'].unique())
        invalid_regions = unique_regions - self.valid_regions
        missing_regions = self.valid_regions - unique_regions
        
        if invalid_regions:
            logger.warning(f"Invalid regions found: {invalid_regions}")
            # Option to remove or correct invalid regions
            self.data = self.data[~self.data['Region'].isin(invalid_regions)]
        
        if missing_regions:
            logger.info(f"Expected regions not present: {missing_regions}")
        
        self.cleaning_report['region_validation'] = {
            'total_regions': len(unique_regions),
            'valid_regions': len(unique_regions & self.valid_regions),
            'invalid_regions': list(invalid_regions),
            'missing_regions': list(missing_regions)
        }
        
        logger.info(f"Region validation completed - {len(unique_regions & self.valid_regions)} valid regions")

    def handle_outliers(self) -> None:
        """
        Detect and handle outliers using statistical methods and domain knowledge.
        """
        logger.info("Handling outliers...")
        
        outliers_handled = {}
        
        for column, (min_val, max_val) in self.valid_ranges.items():
            if column in self.data.columns:
                # Count outliers
                outliers_mask = (self.data[column] < min_val) | (self.data[column] > max_val)
                outliers_count = outliers_mask.sum()
                
                if outliers_count > 0:
                    logger.warning(f"Found {outliers_count} outliers in {column}")
                    
                    # Cap outliers at valid range
                    self.data[column] = self.data[column].clip(lower=min_val, upper=max_val)
                    outliers_handled[column] = int(outliers_count)
        
        # Additional statistical outlier detection using IQR method
        numeric_columns = ['Temperature_C', 'Precipitation_mm', 'Humidity_percent',
                          'Wind_Speed_kmh', 'Atmospheric_Pressure_hPa']
        
        iqr_outliers = {}
        for column in numeric_columns:
            if column in self.data.columns:
                Q1 = self.data[column].quantile(0.25)
                Q3 = self.data[column].quantile(0.75)
                IQR = Q3 - Q1
                
                lower_bound = Q1 - 3 * IQR  # Using 3*IQR for more lenient outlier detection
                upper_bound = Q3 + 3 * IQR
                
                # Count statistical outliers
                stat_outliers = ((self.data[column] < lower_bound) | 
                               (self.data[column] > upper_bound)).sum()
                
                if stat_outliers > 0:
                    iqr_outliers[column] = int(stat_outliers)
        
        self.cleaning_report['outlier_handling'] = {
            'range_outliers_handled': outliers_handled,
            'statistical_outliers_detected': iqr_outliers
        }
        
        logger.info("Outlier handling completed")

    def validate_temperature_relationships(self) -> None:
        """
        Validate logical relationships between temperature columns.
        """
        logger.info("Validating temperature relationships...")
        
        # Check Min <= Avg <= Max temperature
        invalid_min_max = (self.data['Min_Temperature_C'] > self.data['Max_Temperature_C']).sum()
        invalid_avg_min = (self.data['Temperature_C'] < self.data['Min_Temperature_C']).sum()
        invalid_avg_max = (self.data['Temperature_C'] > self.data['Max_Temperature_C']).sum()
        
        total_invalid = invalid_min_max + invalid_avg_min + invalid_avg_max
        
        if total_invalid > 0:
            logger.warning(f"Found {total_invalid} temperature logic violations")
            
            # Fix temperature relationships
            # Ensure Min <= Max
            mask = self.data['Min_Temperature_C'] > self.data['Max_Temperature_C']
            if mask.sum() > 0:
                # Swap min and max where min > max
                temp_min = self.data.loc[mask, 'Min_Temperature_C'].copy()
                self.data.loc[mask, 'Min_Temperature_C'] = self.data.loc[mask, 'Max_Temperature_C']
                self.data.loc[mask, 'Max_Temperature_C'] = temp_min
            
            # Ensure Min <= Avg <= Max
            self.data['Temperature_C'] = np.clip(
                self.data['Temperature_C'],
                self.data['Min_Temperature_C'],
                self.data['Max_Temperature_C']
            )
        
        self.cleaning_report['temperature_validation'] = {
            'invalid_min_max': int(invalid_min_max),
            'invalid_avg_min': int(invalid_avg_min),
            'invalid_avg_max': int(invalid_avg_max),
            'total_fixed': int(total_invalid)
        }
        
        logger.info("Temperature relationship validation completed")

    def validate_heat_index(self) -> None:
        """
        Validate and recalculate heat index where necessary.
        """
        logger.info("Validating heat index...")
        
        def calculate_heat_index(temp_c: float, humidity: float) -> float:
            """
            Calculate heat index using simplified formula.
            
            Args:
                temp_c: Temperature in Celsius
                humidity: Relative humidity percentage
                
            Returns:
                Heat index in Celsius
            """
            if temp_c < 27 or humidity < 40:
                return temp_c
            
            temp_f = temp_c * 9/5 + 32
            
            # Simplified heat index calculation
            hi_f = (temp_f + humidity) / 2 + (temp_f - 32) * 0.1
            hi_c = (hi_f - 32) * 5/9
            
            return max(temp_c, hi_c)
        
        # Recalculate heat index for validation
        calculated_heat_index = self.data.apply(
            lambda row: calculate_heat_index(row['Temperature_C'], row['Humidity_percent']),
            axis=1
        )
        
        # Check for significant differences
        heat_index_diff = abs(self.data['Heat_Index_C'] - calculated_heat_index)
        significant_diff_count = (heat_index_diff > 5).sum()  # More than 5°C difference
        
        if significant_diff_count > 0:
            logger.warning(f"Found {significant_diff_count} heat index values with >5°C difference")
            # Update heat index where there's significant difference
            mask = heat_index_diff > 5
            self.data.loc[mask, 'Heat_Index_C'] = calculated_heat_index[mask]
        
        self.cleaning_report['heat_index_validation'] = {
            'significant_differences': int(significant_diff_count),
            'max_difference': float(heat_index_diff.max()),
            'mean_difference': float(heat_index_diff.mean())
        }
        
        logger.info("Heat index validation completed")

    def add_derived_features(self) -> None:
        """
        Add useful derived features for analysis.
        """
        logger.info("Adding derived features...")
        
        # Temperature features
        self.data['Temperature_Range'] = (self.data['Max_Temperature_C'] - 
                                         self.data['Min_Temperature_C'])
        
        # Comfort index (simplified)
        self.data['Comfort_Index'] = np.where(
            (self.data['Temperature_C'].between(18, 28)) & 
            (self.data['Humidity_percent'].between(40, 70)),
            1, 0
        )
        
        # Season classification
        season_map = {1: 'Winter', 2: 'Winter', 3: 'Spring', 4: 'Summer',
                     5: 'Summer', 6: 'Summer', 7: 'Monsoon', 8: 'Monsoon',
                     9: 'Monsoon', 10: 'Post-Monsoon', 11: 'Post-Monsoon', 12: 'Winter'}
        self.data['Season'] = self.data['Month'].map(season_map)
        
        # Weather severity index
        self.data['Weather_Severity'] = (
            (self.data['Temperature_C'] > 35).astype(int) +
            (self.data['Precipitation_mm'] > 50).astype(int) +
            (self.data['Wind_Speed_kmh'] > 30).astype(int) +
            (self.data['Heat_Index_C'] > 40).astype(int)
        )
        
        # Pressure category
        self.data['Pressure_Category'] = pd.cut(
            self.data['Atmospheric_Pressure_hPa'],
            bins=[0, 1000, 1020, float('inf')],
            labels=['Low', 'Normal', 'High']
        )
        
        logger.info("Derived features added successfully")

    def generate_quality_report(self) -> Dict:
        """
        Generate a comprehensive data quality report.
        
        Returns:
            Dict: Comprehensive quality metrics
        """
        logger.info("Generating quality report...")
        
        quality_metrics = {
            'dataset_overview': {
                'total_records': len(self.data),
                'total_columns': len(self.data.columns),
                'memory_usage_mb': float(self.data.memory_usage(deep=True).sum() / 1024**2),
                'date_range': f"{self.data['Date'].min().date()} to {self.data['Date'].max().date()}",
                'unique_regions': self.data['Region'].nunique()
            },
            'data_completeness': {
                'missing_values': int(self.data.isnull().sum().sum()),
                'completeness_percentage': float((1 - self.data.isnull().sum().sum() / 
                                                (len(self.data) * len(self.data.columns))) * 100)
            },
            'data_quality_indicators': {
                'temperature_logic_errors': int(
                    (self.data['Min_Temperature_C'] > self.data['Max_Temperature_C']).sum()
                ),
                'extreme_temperatures': int(
                    ((self.data['Temperature_C'] < -10) | 
                     (self.data['Temperature_C'] > 50)).sum()
                ),
                'extreme_rainfall': int((self.data['Precipitation_mm'] > 300).sum()),
                'zero_humidity': int((self.data['Humidity_percent'] <= 0).sum()),
                'extreme_winds': int((self.data['Wind_Speed_kmh'] > 100).sum())
            }
        }
        
        # Add cleaning report
        quality_metrics.update(self.cleaning_report)
        
        return quality_metrics

    def save_cleaned_data(self) -> bool:
        """
        Save the cleaned data to CSV file.
        
        Returns:
            bool: True if saved successfully, False otherwise
        """
        try:
            logger.info(f"Saving cleaned data to: {self.output_file}")
            
            # Ensure output directory exists
            self.output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # Save cleaned data
            self.data.to_csv(self.output_file, index=False)
            
            logger.info(f"Successfully saved {len(self.data)} cleaned records")
            return True
            
        except Exception as e:
            logger.error(f"Error saving data: {e}")
            return False

    def run_cleaning_pipeline(self) -> bool:
        """
        Run the complete data cleaning pipeline.
        
        Returns:
            bool: True if pipeline completed successfully, False otherwise
        """
        logger.info("Starting comprehensive data cleaning pipeline...")
        
        try:
            # Load data
            if not self.load_data():
                return False
            
            # Validate structure
            if not self.validate_data_structure():
                return False
            
            # Execute cleaning steps
            self.handle_missing_values()
            self.validate_date_format()
            self.validate_regions()
            self.handle_outliers()
            self.validate_temperature_relationships()
            self.validate_heat_index()
            self.add_derived_features()
            
            # Generate quality report
            quality_report = self.generate_quality_report()
            
            # Save cleaned data
            if not self.save_cleaned_data():
                return False
            
            # Print summary report
            self._print_summary_report(quality_report)
            
            logger.info("Data cleaning pipeline completed successfully!")
            return True
            
        except Exception as e:
            logger.error(f"Error in cleaning pipeline: {e}")
            return False

    def _print_summary_report(self, quality_report: Dict) -> None:
        """Print a formatted summary report."""
        print("\n" + "="*80)
        print("DATA CLEANING SUMMARY REPORT")
        print("="*80)
        
        overview = quality_report['dataset_overview']
        print(f"📊 Dataset Overview:")
        print(f"   • Total Records: {overview['total_records']:,}")
        print(f"   • Total Columns: {overview['total_columns']}")
        print(f"   • Memory Usage: {overview['memory_usage_mb']:.2f} MB")
        print(f"   • Date Range: {overview['date_range']}")
        print(f"   • Unique Regions: {overview['unique_regions']}")
        
        completeness = quality_report['data_completeness']
        print(f"\n🔍 Data Completeness:")
        print(f"   • Missing Values: {completeness['missing_values']}")
        print(f"   • Completeness: {completeness['completeness_percentage']:.2f}%")
        
        if 'missing_values' in self.cleaning_report:
            missing_info = self.cleaning_report['missing_values']
            print(f"   • Missing Values Handled: {missing_info['missing_handled']}")
        
        if 'outlier_handling' in self.cleaning_report:
            outlier_info = self.cleaning_report['outlier_handling']
            if outlier_info['range_outliers_handled']:
                print(f"\n⚠️  Outliers Handled:")
                for col, count in outlier_info['range_outliers_handled'].items():
                    print(f"   • {col}: {count} outliers capped")
        
        if 'temperature_validation' in self.cleaning_report:
            temp_info = self.cleaning_report['temperature_validation']
            if temp_info['total_fixed'] > 0:
                print(f"\n🌡️  Temperature Logic Issues Fixed: {temp_info['total_fixed']}")
        
        quality_indicators = quality_report['data_quality_indicators']
        print(f"\n✅ Data Quality Indicators:")
        print(f"   • Temperature Logic Errors: {quality_indicators['temperature_logic_errors']}")
        print(f"   • Extreme Temperature Records: {quality_indicators['extreme_temperatures']}")
        print(f"   • Extreme Rainfall Records: {quality_indicators['extreme_rainfall']}")
        
        print(f"\n💾 Output File: {self.output_file}")
        print("="*80)


def main():
    """
    Main function to execute the weather data cleaning process.
    """
    print("🌦️  Weather Data Cleaning Script")
    print("="*50)
    
    # Define file paths
    input_file = "comprehensive_indian_weather_data_2024_all_states_uts.csv"
    output_file = "comprehensive_indian_weather_data_2024_all_states_uts_cleaned.csv"
    
    # Initialize cleaner
    cleaner = WeatherDataCleaner(input_file, output_file)
    
    # Run cleaning pipeline
    success = cleaner.run_cleaning_pipeline()
    
    if success:
        print("\n✅ Data cleaning completed successfully!")
        print(f"📁 Cleaned data saved to: {output_file}")
    else:
        print("\n❌ Data cleaning failed. Please check the logs for details.")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
